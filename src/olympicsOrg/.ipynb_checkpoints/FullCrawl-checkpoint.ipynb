{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new plan, related to athlete crawling...\n",
    "# I want to crawl through all relevant pages:\n",
    "# games, sports, events (ind and team), and athletes\n",
    "\n",
    "# another idea to fill in the db: take all the names (first and last individually)\n",
    "# from the olympicdatabase site, and use those as search terms in olympic.org site. yay!\n",
    "# do this after the full crawl for two reasons: both as a QC and way to complete the database,\n",
    "# but also to see how well my algorithm did!\n",
    "\n",
    "# when I start actually parsing results, I should have periodic dumps to file\n",
    "# so the list of results doesn't grow out of control\n",
    "# like, maybe every 100 results or something\n",
    "\n",
    "# one of the errors that came up was from spaces in the link. Need to use urllib2.quote()\n",
    "\n",
    "# info I have available:\n",
    "# data about olympics in olympicsTable\n",
    "# data about sports in sportsTable\n",
    "# data about countries in countryTable\n",
    "#\n",
    "# data I will collect:\n",
    "# event results in resultTable\n",
    "# athlete data in AthleteTable\n",
    "\n",
    "# also an effing different results style here:\n",
    "# /london-2012/cycling-track/keirin-men\n",
    "\n",
    "# need to redo olympics table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "reload(logging)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up imports here\n",
    "import urllib2\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Tag\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import parse_functions as par\n",
    "import os.path\n",
    "from TimeoutClass import Timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define page types\n",
    "pOlympics = \"olympics page\"\n",
    "pSport = \"sport page\"\n",
    "pEvent = \"individual event results page\"\n",
    "pTeamEvent = \"team event results page\"\n",
    "pAthlete = \"athlete page\"\n",
    "pOther = \"other page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine the type of page we are on\n",
    "# take in a soup object and parse to determine \n",
    "# the type of page\n",
    "def determine_page_type(soup):\n",
    "    # use the page title (in head) to check the type\n",
    "    title = pageSoup.find(\"title\").string\n",
    "    \n",
    "    if re.match(re.compile(\".*\\d{4} \\w{6} Olympics - results.*\"),title):\n",
    "        return pOlympics\n",
    "    \n",
    "    # see if it is an athlete page\n",
    "    idSection = soup.find(\"section\",class_=\"id-card\")\n",
    "    if idSection:\n",
    "        idClass = idSection['class']\n",
    "        if len(idClass)==3 and re.match(re.compile(\"noc-\\w{3}\"),idClass[2]):\n",
    "            return pAthlete\n",
    "    \n",
    "    return pOther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def country_from_name(name):\n",
    "    for c in countryTable:\n",
    "        if countryTable[c]['name'].lower() == name.lower():\n",
    "            return c\n",
    "\n",
    "def sport_from_name(name):\n",
    "    for c in sportsTable:\n",
    "        if sportsTable[c]['name'].lower() == name.lower():\n",
    "            return c        \n",
    "        \n",
    "def olympics_from_name(name):\n",
    "    return name\n",
    "    for c in olympicsTable:\n",
    "        if olympicsTable[c]['name'].lower() == name.lower():\n",
    "            return c     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output files at regular intervals\n",
    "def file_output(file_name, data):\n",
    "    logger.info(\"Writing \" + str(len(data)) + \" to \" + file_name)\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4, sort_keys=True, separators=(',', ':'))\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load all the sports\n",
    "with open('./SportsTable.json') as data_file:\n",
    "    sportsTable = json.load(data_file)\n",
    "with open('./CountryIDTable.json') as data_file:\n",
    "    countryIDTable = json.load(data_file)\n",
    "with open('./CountryTable.json') as data_file:\n",
    "    countryTable = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions to check for and load previous states\n",
    "\n",
    "def check_for_file_list(filename):\n",
    "    out = []\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename) as data_file:\n",
    "            out = json.load(data_file)   \n",
    "            logger.info(\"Loaded list file \" + filename)\n",
    "    else:\n",
    "        logger.info(\"Created new list for file \" + filename)\n",
    "    return out\n",
    "\n",
    "def check_for_file_dict(filename):\n",
    "    out = {}\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename) as data_file:\n",
    "            out = json.load(data_file)   \n",
    "            logger.info(\"Loaded list file \" + filename)\n",
    "    else:\n",
    "        logger.info(\"Created new list for file \" + filename)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# define logging output\n",
    "logging.basicConfig(filename='./FullCrawler.log',filemode='w',level=logging.DEBUG)\n",
    "logger.info(\"--------------------------------------------------\")\n",
    "logger.info(\"--------------------------------------------------\")\n",
    "logger.info(\"Starting FullCrawler: \" + str(datetime.now()))\n",
    "\n",
    "# base url\n",
    "baseUrl = \"https://www.olympic.org\"\n",
    "\n",
    "# constants for link tracking\n",
    "VISITED = \"visited\"\n",
    "PROBLEM = \"problem\"\n",
    "\n",
    "# seed with top level page\n",
    "linksToVisit = check_for_file_list(\"LinksToVisit.json\")\n",
    "if len(linksToVisit) == 0:\n",
    "    linksToVisit.append('/')\n",
    "    logger.info(\"Seeding crawler with \" + str(linksToVisit))\n",
    "\n",
    "# count variables\n",
    "interval = 1000\n",
    "max_cnt = 1000000\n",
    "\n",
    "# initialize output lists\n",
    "# load them if available\n",
    "visited = check_for_file_dict(\"Visited.json\")\n",
    "athletes = {}\n",
    "results = []\n",
    "rankings = []\n",
    "\n",
    "# result counters\n",
    "cnt = len(visited)\n",
    "\n",
    "# big loop\n",
    "while len(linksToVisit) > 0 and cnt<max_cnt:\n",
    "    link = linksToVisit.pop()\n",
    "    visited[link] = VISITED\n",
    "    cnt = cnt+1\n",
    "    if cnt%100 == 0:\n",
    "        print cnt\n",
    "        nVisits = len([k for k,v in visited.iteritems() if v==VISITED])\n",
    "        nProblems = len([k for k,v in visited.iteritems() if v==PROBLEM])\n",
    "        logger.info(\"Status report - successful visits so far: \" + str(nVisits))\n",
    "        logger.info(\"Status report - # links in to-visit list: \" + str(len(linksToVisit)))\n",
    "        logger.info(\"Status report - problems so far: \" + str(nProblems))\n",
    "    try:\n",
    "        logger.info(\"page \" + link + \": Starting parse\")\n",
    "\n",
    "        # get the html\n",
    "        with Timeout(60):\n",
    "            html = urllib2.urlopen(baseUrl+urllib2.quote(link)).read()\n",
    "\n",
    "        # soup it up\n",
    "        pageSoup = BeautifulSoup(html)\n",
    "\n",
    "        # determine which type of page\n",
    "        pageType = determine_page_type(pageSoup) \n",
    "        logger.info(\"page \" + link + \": page type = \" + pageType)\n",
    "\n",
    "        # parse this page for new links\n",
    "        # adding them to linksToVisit\n",
    "        newLinks = par.parse_page(pageSoup)\n",
    "        if len(newLinks) > 0:\n",
    "            for l in newLinks:\n",
    "                if (not l in visited) and (not l in linksToVisit): \n",
    "                    logger.info(\"---adding link \" + l)\n",
    "                    linksToVisit.append(l)\n",
    "\n",
    "        # try to parse rankings, just returns if none found\n",
    "        newRankings = par.parse_rankings(pageSoup)\n",
    "        if newRankings:\n",
    "            for rn in newRankings:\n",
    "                if (rn['athlete'] == \"team\"):\n",
    "                    rn['country'] = country_from_name(rn['country'])\n",
    "                else:\n",
    "                    if rn['country'] in countryIDTable:\n",
    "                        rn['country'] = countryIDTable[rn['country']]\n",
    "                    else:\n",
    "                        logger.info(\"page \" + link + \": adding new Country code: \" + rn['country'])\n",
    "                        countryIDTable[rn['country']] = \"NEW COUNTRY\"\n",
    "                        \n",
    "            rankings.extend(newRankings)\n",
    "\n",
    "        # if it's an olympics page or an athlete page:\n",
    "        if pageType == pAthlete:\n",
    "            newAthlete = par.parse_athlete_page(pageSoup)\n",
    "\n",
    "            # add new athlete info to athletes list\n",
    "            athID = par.name_to_link(link.split(\"/\")[-1])\n",
    "            athletes[athID] = newAthlete\n",
    "            athletes[athID]['link'] = link\n",
    "            logger.info(\"page \" + link + \": adding new athlete: \" + athID)\n",
    "\n",
    "            # get the results from this athlete page and add it to the results array     \n",
    "            newResults = par.parse_results(pageSoup)\n",
    "            if newResults:\n",
    "                for res in newResults:\n",
    "                    res['athlete'] = athID\n",
    "                    sport = res['sport']\n",
    "\n",
    "                    # add the sport if it's a new one\n",
    "                    if (not sport in sportsTable):\n",
    "                        logger.info(\"page \" + link + \": adding new sport: \" + sport)\n",
    "                        sportsTable[sport] = {}\n",
    "                        sportsTable[sport]['name'] = sport\n",
    "                        sportsTable[sport]['events'] = {}\n",
    "                    event = res['event']\n",
    "\n",
    "                    # add the event if it's a new one\n",
    "                    if (len(sportsTable[sport]['events']) == 0) or (not event in sportsTable[sport]['events']):\n",
    "                        logger.info(\"page \" + link + \": adding new event: \" + sport + \"/\" + event)\n",
    "                        sportsTable[sport]['events'][event] = event\n",
    "\n",
    "                # add new results to array and update result count\n",
    "                logger.info(\"page \" + link + \": parsed \" + str(len(newResults)) + \" new results\")\n",
    "                results.extend(newResults)\n",
    "\n",
    "        logger.info(\"page \" + link + \": done with parse\")\n",
    "\n",
    "    # timeout error catch\n",
    "    except Timeout.Timeout:\n",
    "        logger.warning(\"page \" + link + \": PROBLEM - http Timeout, adding to back of list\")\n",
    "        del visited[link]\n",
    "        linksToVisit.insert(0,link)\n",
    "        print link + \" - PROBLEM: \" + \" http Timeout\"\n",
    "        \n",
    "    # general error handling, log and track\n",
    "    except Exception as err:\n",
    "        logger.warning(\"page \" + link + \": PROBLEM - \" + format(err))\n",
    "        visited[link] = PROBLEM\n",
    "        print link + \" - PROBLEM: \" + format(err)\n",
    "    \n",
    "    \n",
    "    # make log statement every 1000 pages and output files\n",
    "    if cnt%interval == 0:\n",
    "        logger.info(str(cnt) + \" pages visited at \" + str(datetime.now()))\n",
    "    \n",
    "        nextFile = \"data/AthletesTable_\" + str(cnt/interval) + \".json\"\n",
    "        file_output(nextFile, athletes)\n",
    "        athletes = {}\n",
    "            \n",
    "        nextFile = \"data/ResultsTable_\" + str(cnt/interval) + \".json\"\n",
    "        file_output(nextFile, results)\n",
    "        results = []\n",
    "    \n",
    "        nextFile = \"data/RankingsTable_\" + str(cnt/interval) + \".json\"\n",
    "        file_output(nextFile, rankings)\n",
    "        rankings = []\n",
    "        \n",
    "        nextFile = \"LinksToVisit.json\"\n",
    "        file_output(nextFile,linksToVisit)\n",
    "        \n",
    "        nextFile = \"Visited.json\"\n",
    "        file_output(nextFile,visited)\n",
    "        \n",
    "        nextFile = \"CountryIDTable.json\"\n",
    "        file_output(nextFile,countryIDTable)\n",
    "        \n",
    "        nextFile = \"SportsTable.json\"\n",
    "        file_output(nextFile,sportsTable)\n",
    "    \n",
    "logger.info(\"Athlete crawl complete: \" + str(datetime.now()))\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# had problem here: /london-2012/cycling-track/team-sprint-women\n",
    "# --- this is a team page with bracket style team results\n",
    "# also issue with multiple copies of a link in the linksToVisit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./Visited.json') as data_file:\n",
    "    visited = json.load(data_file)\n",
    "with open('./LinksToVisit.json') as data_file:\n",
    "    linksToVisit = json.load(data_file)    \n",
    "linksToVisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Grenoble 1968 5000m men - Olympic Speed skating\"\n",
    "ol = par.name_to_link(re.compile(\"^\\D+\\d{4}\").findall(title)[0])\n",
    "ev = par.name_to_link(re.split(\"^\\D+\\d{4}\",title)[1].split(\"-\")[0])\n",
    "\n",
    "print ol\n",
    "print ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "x.append(1)\n",
    "x.append(2)\n",
    "x.insert(0,3)\n",
    "print x.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
